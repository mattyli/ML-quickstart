{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "The concept of machine learning and artificial intelligence is highly dependent on the quantity and quality of data that you have. It is this data that is used to train and evaluate your models.\n",
    "\n",
    "Thus, it is important to ensure that your data is 'clean', i.e. there are no missing values, values are of the correct type.\n",
    "\n",
    "Python makes it extremely easy to perform this task with the wide variety of libraries available. For this stage in ML development, the most important libaries are NumPy, Pandas and SKlearn (SciKitLearn)\n",
    "\n",
    "First we will import the libraries and read in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "myData = pd.read_csv(\"Data.csv\")\n",
    "print(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must split the data frame into dependent and indepent values (the information given and what we are trying to predict). In this scenario, given the country, age and salary, we want to predict whether a consumer purchases a product. \n",
    "\n",
    "Thus, whether the consumer PURCHASED something is dependent on the COUNTRY, AGE, and SALARY. \n",
    "\n",
    "Using the .iloc() function of Pandas, we can separate the original dataframe into subdata frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "xSet = myData.iloc[:, :-1].values       # for a slice object, the start:end is exclusive, meaning it will only go up to end-1 where end is an int\n",
    "ySet = myData.iloc[:, -1].values               # iloc[rows, cols]\n",
    "print(ySet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are some values that are missing from the Age and Salary columns, denoted by NaN (Not a number). \n",
    "\n",
    "There are many ways of rectifying this, but one common way is to populate these cells with the averages of the rest in each cell's respective column.\n",
    "Using SKlearn, we can modify the dataframe to do this rectification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")         # scan for missing values (NaN) and use the \"mean\" replacement strategy\n",
    "imputer.fit(xSet[: , 1:3])                                              # slice object refers to the overarching dataframe\n",
    "xSet[:, 1:3] = imputer.transform(xSet[:,1:3])                           # updating the xSet\n",
    "print(xSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in regular english:\n",
    "\n",
    "First we import the SimpleIputer class from the sklearn.impute library and initialize it with two conditions: what kind of missing values it should look for and how it should replace these missing values. In this case we want to replace missing values that are Not A Number (np.nan) and use the \"mean\" strategy (fill in the missing values with the average of all the other values within the specific column).\n",
    "\n",
    "Then we fit the instance of the imputer onto the set we want to fix, in this case it is the XSet and the columns that have numerical values. We then update xSet by using the transform() method of the SimpleImputer class which takes in the specific rows and columns as an arguement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding The Categorical Data\n",
    "\n",
    "Machine learning is all about math, which we can't do on categorical values such as country names (in this case). \n",
    "\n",
    "Instead, we can use one-hot encoding to assign binary vectors to each unique entry in the country column or any categorical volume for that. \n",
    "\n",
    "Binary (0's and 1's) can be used to assign a unique identity to each entry. Since there are 3 distinct countries we could represent them as numbers: [1, 2, 3].\n",
    "\n",
    "In binary: [000, 001, 010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "columnTransformer = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we intialize a columnTransformer object of the ColumnTransformer class by giving it 2 arguements: transformers and remainders.\n",
    "\n",
    "transformers takes in the method of transforming, an instance of the transformer class we want to provide and the columns we want to transform. Remainder is everything we watnt to leave untouched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "xSet = np.array(columnTransformer.fit_transform(xSet))\n",
    "\n",
    "# applying the fit transform and type casting the output to a numpy array\n",
    "# necessary for later\n",
    "\n",
    "print(xSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when encoding a field with multiple unique values N (N > 2) we must use one hot encoding, it is not enough to assign a state of 0 or 1.\n",
    "\n",
    "But for labels such as yes or no, you CAN just represent them with 0's and 1's. And that it is what we will do for the dependent variables, ySet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()    # intializing an object of the Label Encoder Class\n",
    "ySet = encoder.fit_transform(ySet)\n",
    "\n",
    "print(ySet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that LabelEncoder should just be used for encoding the dependent variables (Y). sklearn should remind you of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training and Test set\n",
    "\n",
    "Now we must create two separate sets, one to train the actual model on and another to evaluate model performance on. \n",
    "\n",
    "We can use a sklearn function to split the intial data set into the two desired sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xSet, ySet, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the train_test_split() gives four matrices and takes in 4 arguements.\n",
    "\n",
    "Obviously we must pass in the independent and dependent matrices, but we must also pass in a split size, as in what percentage of the original data should be reserved for training and testing. The random_state just sets the random state to constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "To ensure that other features don't dominate others, it is important to ensure that all features are on the same scale. Remember that this must always be done AFTER splitting the data into a test and training set.\n",
    "\n",
    "There are two methods of feature scaling: standardisation and normalisation. \n",
    "\n",
    "Just remember that standardisation results in values between +/- 3 and normalisation results in values between 0-1.\n",
    "\n",
    "Depending on the shape of your data, one method may work best. As the name implies, if your data exhibits a normal distribution, normalisation is best.\n",
    "\n",
    "Standardisation works well in any situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, -0.6546536707079772, 0.7588743615900191,\n",
       "        0.7494732544921673],\n",
       "       [1.0, 0.0, 0.0, 1.5275252316519468, -1.7115038793306814,\n",
       "        -1.4381784072687536],\n",
       "       [1.0, 0.0, 1.0, -0.6546536707079772, -1.2755547779917342,\n",
       "        -0.8912654918285233],\n",
       "       [1.0, 0.0, 0.0, 1.5275252316519468, -0.11302384108787522,\n",
       "        -0.25320042381492147],\n",
       "       [1.0, 0.0, 1.0, -0.6546536707079772, 0.17760889313808953,\n",
       "        2.357833340847479e-16],\n",
       "       [0.0, 1.0, 0.0, -0.6546536707079772, -0.5489729424268224,\n",
       "        -0.5266568815350365],\n",
       "       [1.0, 0.0, 0.0, 1.5275252316519468, 8.881784197001253e-17,\n",
       "        -1.0735697969752667],\n",
       "       [0.0, 1.0, 0.0, -0.6546536707079772, 1.3401398300419485,\n",
       "        1.3875383225057691],\n",
       "       [1.0, 0.0, 1.0, -0.6546536707079772, 1.6307725642679132,\n",
       "        1.752146932799256],\n",
       "       [0.0, 1.0, 0.0, -0.6546536707079772, -0.25834020820085757,\n",
       "        0.2937124916253087]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler        # importing the scaler\n",
    "sc = StandardScaler()                                   # creating an instance of the class\n",
    "# DO NOT APPLY FEATURE SCALING ON DUMMY VARIABLES\n",
    "\n",
    "xSet[:,3:] = sc.fit_transform(xSet[:, 3:])              # applying scaling\n",
    "xTest[:,3:] = sc.transform(xTest[:,3:])             # don't need to fit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "880c40a315a9aad5eacc1d65763e70cdbad02f85abc026da352346917c350176"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
